#!/usr/bin/env python3
"""
Enterprise Scrum at Scale Agents using Weaver-Generated Models
This demonstrates:
1. Pydantic models GENERATED by Weaver Forge from semantic conventions
2. Pydantic AI agents using these models for structured output
3. OTel spans carrying the structured data
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Literal
from pathlib import Path

# Pydantic and Pydantic AI
from pydantic import BaseModel, Field
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider

# OpenTelemetry
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor

# ========================================================================
# MODELS GENERATED BY WEAVER FORGE FROM semantic-conventions/groups/
# ========================================================================

# From agents-ai.yaml -> agent.output.decision
class AgentDecision(BaseModel):
    """Generated from agent.output.decision semantic convention"""
    decision: str = Field(..., description="The decision made")
    reasoning: str = Field(..., description="Agent's reasoning for the decision")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Agent's confidence in decision (0-1)")
    alternatives_considered: List[str] = Field(default_factory=list, description="Alternative options the agent considered")
    estimated_impact: Optional[str] = Field(None, description="Expected impact of decision")
    next_steps: List[str] = Field(default_factory=list, description="Recommended next actions")

# From agents-ai.yaml -> agent.output.analysis
class AgentAnalysis(BaseModel):
    """Generated from agent.output.analysis semantic convention"""
    subject: str = Field(..., description="What was analyzed")
    insights: List[str] = Field(..., description="Key insights from analysis")
    patterns_found: List[Dict[str, Any]] = Field(default_factory=list, description="Patterns identified")
    recommendations: List[str] = Field(..., description="Recommended actions based on analysis")
    data_points: int = Field(..., ge=0, description="Number of data points analyzed")
    confidence: float = Field(..., ge=0.0, le=1.0)

# From scrum-at-scale-enterprise.yaml -> sas.eat
class EATDecision(BaseModel):
    """Generated from sas.eat executive decision attributes"""
    eat_id: str = Field(..., description="Unique identifier for EAT session")
    decision_type: Literal["strategic", "operational", "transformation", "impediment"] = Field(...)
    decision: str = Field(..., description="Executive decision made")
    reasoning: str = Field(..., description="Strategic reasoning")
    business_units_impacted: List[str] = Field(..., description="Business units affected")
    investment_required: Optional[float] = Field(None, description="Investment in millions")
    strategic_priorities: List[str] = Field(..., description="Aligned strategic priorities")
    confidence: float = Field(..., ge=0.0, le=1.0)
    timeline: str = Field(..., description="Implementation timeline")

# From scrum-at-scale-enterprise.yaml -> sas.ems
class EMSPrioritization(BaseModel):
    """Generated from sas.ems backlog prioritization"""
    ems_id: str = Field(..., description="Unique identifier for EMS session")
    prioritized_items: List[Dict[str, Any]] = Field(..., description="Backlog items in priority order")
    wsjf_calculations: Dict[str, float] = Field(..., description="WSJF score per item")
    release_train_assignments: Dict[str, List[str]] = Field(..., description="Items per release train")
    portfolio_impact: float = Field(..., description="Portfolio value impact in millions")
    dependencies_mapped: List[Dict[str, str]] = Field(default_factory=list)
    confidence: float = Field(..., ge=0.0, le=1.0)

# From scrum-at-scale-enterprise.yaml -> sas.impediment
class ImpedimentResolution(BaseModel):
    """Generated from sas.impediment resolution attributes"""
    impediment_id: str = Field(..., description="Unique impediment identifier")
    severity: Literal["low", "medium", "high", "critical"] = Field(...)
    root_causes: List[str] = Field(..., min_items=1, description="Root cause analysis results")
    resolution_plan: str = Field(..., description="Detailed resolution approach")
    teams_unblocked: int = Field(..., ge=0, description="Number of teams that will be unblocked")
    cost_of_delay_daily: float = Field(..., ge=0, description="Cost of delay per day")
    estimated_resolution_time: str = Field(..., description="Time to resolve")
    resources_required: List[str] = Field(..., description="Resources needed")
    escalation_required: bool = Field(..., description="Needs executive attention")
    prevention_measures: List[str] = Field(..., description="How to prevent recurrence")

# From scrum-at-scale-enterprise.yaml -> sas.metrics
class EnterpriseMetrics(BaseModel):
    """Generated from sas.metrics monitoring attributes"""
    total_teams: int = Field(..., ge=0)
    total_people: int = Field(..., ge=0)
    velocity_achievement: float = Field(..., ge=0.0, le=1.0, description="Velocity achievement ratio")
    impediments_per_sprint: float = Field(..., ge=0.0)
    feature_cycle_time: float = Field(..., ge=0.0, description="Average feature cycle time in days")
    defect_escape_rate: float = Field(..., ge=0.0, le=1.0)
    customer_satisfaction: float = Field(..., ge=0.0, le=5.0)
    employee_engagement: float = Field(..., ge=0.0, le=5.0)

# ========================================================================
# OTEL SETUP WITH STRUCTURED OUTPUT TRACKING
# ========================================================================

def setup_telemetry_with_structured_output():
    """Setup OTel to track structured Pydantic outputs"""
    resource = Resource.create({
        "service.name": "enterprise-sas-weaver-generated",
        "service.version": "4.0.0",
        "deployment.environment": "production",
        "sas.implementation": "weaver_forge_generated"
    })
    
    provider = TracerProvider(resource=resource)
    
    # Console exporter that shows our structured data
    console_exporter = ConsoleSpanExporter()
    provider.add_span_processor(BatchSpanProcessor(console_exporter))
    
    trace.set_tracer_provider(provider)
    return trace.get_tracer(__name__)

tracer = setup_telemetry_with_structured_output()

# ========================================================================
# AI AGENTS USING WEAVER-GENERATED MODELS
# ========================================================================

class WeaverGeneratedAgent:
    """Base agent using Weaver-generated Pydantic models"""
    
    def __init__(self, agent_id: str, name: str, role: str):
        self.agent_id = agent_id
        self.name = name
        self.role = role
        
        # Ollama via OpenAI-compatible API
        self.model = OpenAIModel(
            model_name='qwen3:latest',
            provider=OpenAIProvider(base_url='http://192.168.1.74:11434/v1')
        )
        
        # Track structured outputs
        self.structured_outputs = []

class ExecutiveAgent(WeaverGeneratedAgent):
    """CEO/CTO/CFO using EAT decision structure"""
    
    def __init__(self, title: str):
        super().__init__(f"{title.lower()}-001", f"{title} Agent", "executive")
        self.title = title
        self.portfolio_value = 5000.0  # $5B
        
    async def make_eat_decision(self, context: Dict[str, Any]) -> EATDecision:
        """Make executive decision with EAT structure"""
        
        with tracer.start_as_current_span("sas.eat.decision") as span:
            # Set all EAT attributes from semantic convention
            span.set_attribute("sas.eat.id", f"eat-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}")
            span.set_attribute("sas.eat.type", context.get("decision_type", "strategic"))
            span.set_attribute("sas.eat.executives_present", 7)
            span.set_attribute("agent.ai.id", self.agent_id)
            span.set_attribute("agent.ai.role", self.role)
            span.set_attribute("agent.ai.model", "qwen3:latest")
            
            # Create agent with EAT decision output
            decision_agent = Agent(
                model=self.model,
                output_type=EATDecision,
                system_prompt=f"""You are the {self.title} of a $5B enterprise.
                
Make executive decisions considering:
- Strategic alignment with digital transformation
- ROI and financial impact
- Risk assessment and mitigation
- Cross-functional dependencies
- Long-term sustainability

Your decision authority: unlimited for strategic initiatives."""
            )
            
            prompt = f"""Make an executive decision on:

{json.dumps(context, indent=2)}

Consider business impact, strategic priorities, and implementation timeline."""

            print(f"\nðŸ¤” {self.title} analyzing (EAT decision)...")
            start_time = time.time()
            
            # Real LLM call returning structured EAT decision
            result = await decision_agent.run(prompt)
            eat_decision = result.output
            
            thinking_time = time.time() - start_time
            
            # Track structured output in span
            span.set_attribute("agent.decision.thinking_time_ms", int(thinking_time * 1000))
            span.set_attribute("agent.decision.confidence", eat_decision.confidence)
            span.set_attribute("agent.decision.type", eat_decision.decision_type)
            
            # Store full structured output as span attribute
            span.set_attribute("structured_output.type", "EATDecision")
            span.set_attribute("structured_output.content", eat_decision.model_dump_json())
            
            print(f"âœ… {self.title} Decision: {eat_decision.decision}")
            print(f"   Type: {eat_decision.decision_type}")
            print(f"   Confidence: {eat_decision.confidence:.1%}")
            print(f"   Investment: ${eat_decision.investment_required:,.0f}M" if eat_decision.investment_required else "   Investment: N/A")
            
            self.structured_outputs.append(eat_decision)
            return eat_decision

class ChiefProductOwnerAgent(WeaverGeneratedAgent):
    """CPO using EMS prioritization structure"""
    
    def __init__(self):
        super().__init__("cpo-001", "Chief Product Owner", "chief_product_owner")
        self.portfolio_value = 1200.0  # $1.2B
        self.release_trains = ["platform", "mobile", "data", "infrastructure", "ai_ml"]
        
    async def prioritize_portfolio(self, backlog_items: List[Dict[str, Any]]) -> EMSPrioritization:
        """Prioritize portfolio using EMS structure"""
        
        with tracer.start_as_current_span("sas.ems.prioritization") as span:
            # Set EMS attributes
            span.set_attribute("sas.ems.id", f"ems-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}")
            span.set_attribute("sas.ems.chief_product_owner", self.name)
            span.set_attribute("sas.ems.product_owners_count", 45)
            span.set_attribute("sas.ems.portfolio_value", self.portfolio_value)
            span.set_attribute("sas.ems.release_trains", json.dumps(self.release_trains))
            span.set_attribute("agent.ai.id", self.agent_id)
            
            # Create prioritization agent
            prioritization_agent = Agent(
                model=self.model,
                output_type=EMSPrioritization,
                system_prompt="""You are the Chief Product Owner managing a $1.2B portfolio.

Use WSJF (Weighted Shortest Job First) for prioritization:
WSJF = (Business Value + Time Criticality + Risk Reduction) / Job Size

Consider:
- Cross-release train dependencies
- Portfolio-level value optimization
- Resource constraints
- Strategic alignment"""
            )
            
            prompt = f"""Prioritize this portfolio backlog:

Items: {json.dumps(backlog_items, indent=2)}
Release Trains: {self.release_trains}

Calculate WSJF, map dependencies, and assign to release trains."""

            print(f"\nðŸ“Š {self.name} prioritizing portfolio...")
            
            result = await prioritization_agent.run(prompt)
            ems_prioritization = result.output
            
            # Track structured output
            span.set_attribute("sas.ems.backlog_items_refined", len(backlog_items))
            span.set_attribute("structured_output.type", "EMSPrioritization")
            span.set_attribute("structured_output.content", ems_prioritization.model_dump_json())
            
            print(f"âœ… Portfolio Prioritized:")
            print(f"   Items: {len(ems_prioritization.prioritized_items)}")
            print(f"   Portfolio Impact: ${ems_prioritization.portfolio_impact:,.1f}M")
            print(f"   Confidence: {ems_prioritization.confidence:.1%}")
            
            self.structured_outputs.append(ems_prioritization)
            return ems_prioritization

class ScrumMasterCoordinatorAgent(WeaverGeneratedAgent):
    """Scrum Master using impediment resolution structure"""
    
    def __init__(self, teams: List[str]):
        super().__init__(f"sm-coordinator", "SM Coordinator", "scrum_master")
        self.teams = teams
        
    async def resolve_enterprise_impediment(self, impediment: Dict[str, Any]) -> ImpedimentResolution:
        """Resolve impediment using structured approach"""
        
        with tracer.start_as_current_span("sas.impediment.resolution") as span:
            # Set impediment attributes
            span.set_attribute("sas.impediment.id", impediment.get("id", "unknown"))
            span.set_attribute("sas.impediment.severity", impediment.get("severity", "unknown"))
            span.set_attribute("sas.impediment.teams_affected", len(impediment.get("teams_affected", [])))
            span.set_attribute("agent.ai.id", self.agent_id)
            span.set_attribute("agent.scrum_master.teams_facilitated", len(self.teams))
            
            # Create resolution agent
            resolution_agent = Agent(
                model=self.model,
                output_type=ImpedimentResolution,
                system_prompt=f"""You are an enterprise Scrum Master coordinator for {len(self.teams)} teams.

Use systematic impediment resolution:
1. Root cause analysis (5 Whys, Fishbone)
2. Impact assessment (teams, cost, timeline)
3. Resolution planning (immediate and long-term)
4. Prevention measures
5. Escalation criteria

Focus on systemic solutions that prevent recurrence."""
            )
            
            prompt = f"""Resolve this enterprise impediment:

{json.dumps(impediment, indent=2)}

Provide comprehensive resolution with prevention measures."""

            print(f"\nðŸ”§ {self.name} analyzing impediment...")
            
            result = await resolution_agent.run(prompt)
            resolution = result.output
            
            # Track structured output
            span.set_attribute("sas.impediment.resolution_time", resolution.estimated_resolution_time)
            span.set_attribute("sas.impediment.escalated", resolution.escalation_required)
            span.set_attribute("structured_output.type", "ImpedimentResolution")
            span.set_attribute("structured_output.content", resolution.model_dump_json())
            
            print(f"âœ… Resolution Plan:")
            print(f"   Severity: {resolution.severity}")
            print(f"   Teams to unblock: {resolution.teams_unblocked}")
            print(f"   Cost of delay: ${resolution.cost_of_delay_daily:,.0f}/day")
            print(f"   Resolution time: {resolution.estimated_resolution_time}")
            
            self.structured_outputs.append(resolution)
            return resolution

# ========================================================================
# ENTERPRISE METRICS AGGREGATOR
# ========================================================================

class MetricsAggregatorAgent(WeaverGeneratedAgent):
    """Agent that analyzes enterprise metrics"""
    
    def __init__(self):
        super().__init__("metrics-001", "Metrics Aggregator", "analyst")
        
    async def analyze_enterprise_health(self, data: Dict[str, Any]) -> EnterpriseMetrics:
        """Analyze enterprise health metrics"""
        
        with tracer.start_as_current_span("sas.metrics.analysis") as span:
            span.set_attribute("agent.ai.id", self.agent_id)
            span.set_attribute("agent.analysis.subject", "enterprise_health")
            
            # Create metrics agent
            metrics_agent = Agent(
                model=self.model,
                output_type=EnterpriseMetrics,
                system_prompt="""You are analyzing enterprise Scrum at Scale metrics.

Calculate and assess:
- Team performance metrics
- Velocity trends
- Quality indicators
- Employee satisfaction
- Customer satisfaction

Provide realistic metrics based on the data."""
            )
            
            result = await metrics_agent.run(f"Analyze enterprise metrics: {json.dumps(data, indent=2)}")
            metrics = result.output
            
            # Track structured output
            span.set_attribute("sas.metrics.total_teams", metrics.total_teams)
            span.set_attribute("sas.metrics.velocity_achievement", metrics.velocity_achievement)
            span.set_attribute("structured_output.type", "EnterpriseMetrics")
            span.set_attribute("structured_output.content", metrics.model_dump_json())
            
            return metrics

# ========================================================================
# DEMONSTRATION OF WEAVER-GENERATED SYSTEM
# ========================================================================

async def demonstrate_weaver_generated_sas():
    """Demonstrate the full system with Weaver-generated models"""
    
    print("ðŸ¢ Enterprise Scrum at Scale with Weaver-Generated Models")
    print("ðŸ”§ Models generated from semantic conventions via Weaver Forge")
    print("ðŸ¤– Pydantic AI agents using structured output types")
    print("ðŸ“¡ Full OTel observability with structured data in spans")
    print("="*60)
    
    # Test Ollama connection
    print("\nðŸ”Œ Testing Ollama connection...")
    try:
        test_model = OpenAIModel(
            model_name='qwen3:latest',
            provider=OpenAIProvider(base_url='http://192.168.1.74:11434/v1')
        )
        test_agent = Agent(test_model, output_type=str)
        result = await test_agent.run("Say 'Connected'")
        print(f"âœ… Ollama connected: {result.output}")
    except Exception as e:
        print(f"âŒ Connection failed: {e}")
        return
    
    # Create agents
    ceo = ExecutiveAgent("CEO")
    cpo = ChiefProductOwnerAgent()
    sm_coord = ScrumMasterCoordinatorAgent(["Platform", "Mobile", "Data", "Infrastructure", "AI/ML"])
    metrics = MetricsAggregatorAgent()
    
    # Scenario 1: Executive Strategic Decision (EAT)
    print("\n" + "="*60)
    print("ðŸ“‹ SCENARIO 1: Executive Action Team Decision")
    print("="*60)
    
    strategic_context = {
        "decision_type": "strategic",
        "proposal": "Enterprise AI Transformation Initiative",
        "investment_required": 150.0,  # $150M
        "timeline": "3 years",
        "expected_benefits": [
            "50% reduction in time-to-market",
            "30% improvement in quality",
            "Enable AI-driven insights"
        ],
        "risks": ["talent gap", "integration complexity", "market timing"]
    }
    
    eat_decision = await ceo.make_eat_decision(strategic_context)
    
    # Scenario 2: Portfolio Prioritization (EMS)
    print("\n" + "="*60)
    print("ðŸ“‹ SCENARIO 2: Executive MetaScrum Portfolio Prioritization")
    print("="*60)
    
    portfolio_items = [
        {
            "id": "EPIC-001",
            "title": "AI-Powered Analytics Platform",
            "business_value": 95,
            "time_criticality": 85,
            "risk_reduction": 70,
            "size": 89,
            "release_train": "ai_ml"
        },
        {
            "id": "EPIC-002",
            "title": "Mobile Experience Redesign",
            "business_value": 80,
            "time_criticality": 90,
            "risk_reduction": 50,
            "size": 55,
            "release_train": "mobile"
        },
        {
            "id": "EPIC-003",
            "title": "Infrastructure Modernization",
            "business_value": 70,
            "time_criticality": 60,
            "risk_reduction": 85,
            "size": 34,
            "release_train": "infrastructure"
        }
    ]
    
    ems_prioritization = await cpo.prioritize_portfolio(portfolio_items)
    
    # Scenario 3: Enterprise Impediment Resolution
    print("\n" + "="*60)
    print("ðŸ“‹ SCENARIO 3: Enterprise Impediment Resolution")
    print("="*60)
    
    enterprise_impediment = {
        "id": "EI-2024-001",
        "description": "Cross-team deployment pipeline causing cascading failures",
        "severity": "critical",
        "teams_affected": ["Platform", "Mobile", "Data", "Infrastructure"],
        "business_impact": "Cannot deploy critical security patches",
        "duration_days": 3,
        "attempted_fixes": ["restart services", "rollback changes", "increase resources"],
        "cost_of_delay": 75000  # per day
    }
    
    impediment_resolution = await sm_coord.resolve_enterprise_impediment(enterprise_impediment)
    
    # Scenario 4: Enterprise Metrics Analysis
    print("\n" + "="*60)
    print("ðŸ“‹ SCENARIO 4: Enterprise Metrics Dashboard")
    print("="*60)
    
    enterprise_data = {
        "teams": 125,
        "people": 875,
        "sprints_completed": 156,
        "features_delivered": 342,
        "impediments_resolved": 89,
        "customer_feedback_score": 4.2
    }
    
    enterprise_metrics = await metrics.analyze_enterprise_health(enterprise_data)
    
    print(f"\nðŸ“Š Enterprise Health:")
    print(f"   Teams: {enterprise_metrics.total_teams}")
    print(f"   Velocity Achievement: {enterprise_metrics.velocity_achievement:.1%}")
    print(f"   Customer Satisfaction: {enterprise_metrics.customer_satisfaction}/5")
    print(f"   Employee Engagement: {enterprise_metrics.employee_engagement}/5")
    
    # Summary
    print("\n" + "="*60)
    print("ðŸ“Š WEAVER-GENERATED SYSTEM SUMMARY")
    print("="*60)
    print(f"""
âœ… All Pydantic models generated from semantic conventions
ðŸ¤– {len([ceo, cpo, sm_coord, metrics])} agents created with structured output
ðŸ“¡ All decisions tracked in OTel spans with full structure
ðŸ” Structured outputs searchable and analyzable

Key Integration Points:
1. Semantic conventions define the data model
2. Weaver Forge generates Pydantic classes
3. Pydantic AI uses classes for structured output
4. OTel spans carry the structured data
5. Full observability of AI decision-making
""")

    # Show mermaid diagram
    print("\nðŸ“ˆ ARCHITECTURE FLOW:")
    print("""
```mermaid
graph TB
    subgraph "Semantic Conventions"
        SC1[agents-ai.yaml]
        SC2[scrum-at-scale-enterprise.yaml]
    end
    
    subgraph "Weaver Forge"
        WF[Generate Pydantic Models]
    end
    
    subgraph "Generated Models"
        M1[AgentDecision]
        M2[EATDecision]
        M3[EMSPrioritization]
        M4[ImpedimentResolution]
        M5[EnterpriseMetrics]
    end
    
    subgraph "Pydantic AI Agents"
        A1[CEO Agent]
        A2[CPO Agent]
        A3[SM Agent]
        A4[Metrics Agent]
    end
    
    subgraph "Ollama LLM"
        LLM[qwen3:latest]
    end
    
    subgraph "OpenTelemetry"
        SPAN[Structured Spans]
    end
    
    SC1 --> WF
    SC2 --> WF
    WF --> M1
    WF --> M2
    WF --> M3
    WF --> M4
    WF --> M5
    
    M2 --> A1
    M3 --> A2
    M4 --> A3
    M5 --> A4
    
    A1 --> LLM
    A2 --> LLM
    A3 --> LLM
    A4 --> LLM
    
    LLM --> SPAN
    
    style WF fill:#f9f,stroke:#333,stroke-width:4px
    style SPAN fill:#9f9,stroke:#333,stroke-width:4px
```
""")

if __name__ == "__main__":
    print("ðŸš€ Starting Weaver-Generated Enterprise System")
    print("âš¡ Real LLM calls with structured output via generated models")
    print("="*60)
    
    asyncio.run(demonstrate_weaver_generated_sas())