import asyncio
import json
import yaml
import importlib.util
from pathlib import Path
import shutil
import subprocess
import os
import time
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax

console = Console()

# --- Configuration ---
TEMP_DIR = Path("./full_cycle_weaver_forge_ollama_workspace")
WEAVER_BINARY_PATH = "/Users/sac/.cargo/bin/weaver" # Confirmed from previous step
OLLAMA_BASE_URL = "http://localhost:11434/v1"
OLLAMA_MODEL = "qwen3:latest"

# --- Step 1: Define Semantic Convention ---
def define_semantic_convention() -> Path:
    console.print(Panel("[bold cyan]Step 1: Defining Semantic Convention[/bold cyan]", expand=False))
    
    semantic_content = {
        "groups": [
            {
                "id": "ollama.agent.interaction",
                "type": "span",
                "prefix": "ollama.agent",
                "brief": "Interaction with an Ollama-powered AI agent.",
                "attributes": [
                    {"id": "prompt", "type": "string", "brief": "The prompt sent to the Ollama agent."},
                    {"id": "response", "type": "string", "brief": "The response received from the Ollama agent."},
                    {"id": "model_name", "type": "string", "brief": "The name of the Ollama model used."},
                    {"id": "timestamp", "type": "string", "brief": "Timestamp of the interaction."},
                ]
            }
        ]
    }
    
    registry_dir = TEMP_DIR / "my_registry"
    registry_dir.mkdir(exist_ok=True)
    
    semantic_file_in_registry = registry_dir / "my_semantic_convention.yaml"
    with open(semantic_file_in_registry, "w") as f:
        yaml.dump(semantic_content, f, indent=2)
        
    manifest_content = {
        "name": "MyWeaverRegistry",
        "semconv_version": "1.0.0",
        "schema_base_url": "https://opentelemetry.io/schemas/",
        "registry_schema_url": "https://opentelemetry.io/schemas/1.21.0",
        "semantic_conventions": [
            {"path": "my_semantic_convention.yaml"}
        ],
        "templates": [
            {
                "type": "python",
                "path": "pydantic_ollama_agent.py.j2",
                "applies_to": "ollama.agent.interaction"
            }
        ]
    }
    manifest_file = registry_dir / "registry_manifest.yaml"
    with open(manifest_file, "w") as f:
        yaml.dump(manifest_content, f, indent=2)

    console.print(f"  [green]âœ“[/green] Created semantic convention at [cyan]{semantic_file_in_registry}[/cyan]")
    console.print(f"  [green]âœ“[/green] Created registry manifest at [cyan]{manifest_file}[/cyan]")
    console.print(Syntax(yaml.dump(semantic_content), "yaml", theme="monokai", line_numbers=True))
    return registry_dir

# --- Step 2: Create Jinja2 Template ---
def create_jinja2_template() -> Path:
    console.print(Panel("[bold cyan]Step 2: Creating Jinja2 Template[/bold cyan]", expand=False))
    
    import textwrap
    template_content = textwrap.dedent('''
        # This file is generated by Weaver Forge from semantic conventions
        # It uses Pydantic AI to interact with Ollama

        import asyncio
        import os
        from datetime import datetime
        from pydantic import BaseModel, Field
        from pydantic_ai import Agent
        from pydantic_ai.models.openai import OpenAIModel
        from pydantic_ai.providers.openai import OpenAIProvider

        # Configure Ollama connection
        os.environ["OPENAI_API_KEY"] = "ollama"
        os.environ["OPENAI_BASE_URL"] = "{ollama_base_url_placeholder}"

        # Define a Pydantic model for the agent's output, based on semantic convention
        class OllamaAgentResponse(BaseModel):
            prompt: str = Field(..., description="The prompt sent to the Ollama agent.")
            response: str = Field(..., description="The response received from the Ollama agent.")
            model_name: str = Field(..., description="The name of the Ollama model used.")
            timestamp: str = Field(..., description="Timestamp of the interaction.")

        # Define the AI agent
        class MyOllamaAgent:
            def __init__(self, model_name: str = "{ollama_model_placeholder}"):
                self.model_name = model_name
                self.ollama_model = OpenAIModel(
                    model_name=self.model_name,
                    provider=OpenAIProvider(base_url="{ollama_base_url_placeholder}")
                )
                self.agent = Agent(
                    self.ollama_model,
                    result_type=OllamaAgentResponse,
                    system_prompt="You are a helpful AI assistant that provides concise answers."
                )

            async def interact(self, user_prompt: str) -> OllamaAgentResponse:
                print(f"    [bright_magenta]â–¶ Agent interacting with Ollama model: {{self.model_name}}[/bright_magenta]")
                response_obj = await self.agent.run(user_prompt)
                response_obj.output.prompt = user_prompt # Ensure prompt is captured
                response_obj.output.model_name = self.model_name # Ensure model_name is captured
                response_obj.output.timestamp = datetime.utcnow().isoformat() # Ensure timestamp is captured
                print(f"    [bright_blue]âœ“ Agent received response: {{response_obj.output.response[:50]}}...[/bright_blue]")
                return response_obj.output

        # Main function to run the agent interaction
        async def run_ollama_interaction(user_prompt: str) -> OllamaAgentResponse:
            agent = MyOllamaAgent()
            return await agent.interact(user_prompt)

        if __name__ == "__main__":
            # Example usage when run directly (for testing template output)
            asyncio.run(run_ollama_interaction("What is the capital of France?"))
    ''').format(ollama_base_url_placeholder=OLLAMA_BASE_URL, ollama_model_placeholder=OLLAMA_MODEL)
    
    template_dir = TEMP_DIR / "my_templates_root" / "registry" / "python"
    template_dir.mkdir(parents=True, exist_ok=True)
    template_file = template_dir / "pydantic_ollama_agent.py.j2"
    with open(template_file, "w") as f:
        f.write(template_content)
        
    console.print(f"  [green]âœ“[/green] Created Jinja2 template at [cyan]{template_file}[/cyan]")
    console.print(Syntax(template_content, "jinja2", theme="monokai", line_numbers=True))
    return template_file

# --- Step 3: Run Weaver Forge ---
def run_weaver_forge(semantic_file: Path, template_file: Path) -> Path:
    console.print(Panel("[bold cyan]Step 3: Running Weaver Forge[/bold cyan]", expand=False))
    
    output_dir = TEMP_DIR / "generated_code"
    output_dir.mkdir(exist_ok=True)
    
    # Weaver command to generate code using 'forge generate'
    command = [
        WEAVER_BINARY_PATH,
        "forge",
        "generate",
        str(template_file), # Template file directly
        str(semantic_file), # Semantic file directly
        "--output", str(output_dir / "pydantic_ollama_agent.py") # Explicit output file
    ]
    
    console.print(f"  [yellow]Running Weaver command:[/yellow] {' '.join(command)}")
    
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        console.print(f"""  [green]âœ“[/green] Weaver Forge output:
{result.stdout}""")
        if result.stderr:
            console.print(f"""  [yellow]Weaver Forge stderr:
{result.stderr}[/yellow]""")
            
        generated_file = output_dir / "pydantic_ollama_agent.py" # Weaver names it after the template
        if not generated_file.exists():
            raise FileNotFoundError(f"Weaver Forge did not generate expected file: {generated_file}")
            
        console.print(f"  [green]âœ“[/green] Code generated by Weaver Forge at [cyan]{generated_file}[/cyan]")
        console.print(Syntax(generated_file.read_text(), "python", theme="monokai", line_numbers=True))
        return generated_file
        
    except subprocess.CalledProcessError as e:
        console.print(f"""  [red]âœ—[/red] Weaver Forge failed with error:
{e.stderr}""")
        raise
    except Exception as e:
        console.print(f"  [red]âœ—[/red] An unexpected error occurred during Weaver Forge execution: {e}")
        raise

# --- Step 4: Execute Generated Code ---
async def execute_generated_code(generated_file: Path) -> dict:
    console.print(Panel("[bold cyan]Step 4: Executing Generated Code[/bold cyan]", expand=False))
    
    try:
        # Add generated code directory to Python path for import
        import sys
        sys.path.insert(0, str(generated_file.parent))
        
        spec = importlib.util.spec_from_file_location("generated_agent_module", generated_file)
        generated_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(generated_module)
        
        user_prompt = "Tell me a short story about a brave knight."
        console.print(f'  [yellow]Prompting generated agent:[/yellow] "{user_prompt}"' )
        
        # Run the interaction and get the structured response
        response = await generated_module.run_ollama_interaction(user_prompt)
        
        console.print(f"  [green]âœ“[/green] Generated agent executed successfully.")
        return response.dict()
        
    except Exception as e:
        console.print(f"  [red]âœ—[/red] Failed to execute generated code or interact with Ollama: {e}")
        raise

# --- Step 5: Validate Output ---
def validate_output(response: dict) -> bool:
    console.print(Panel("[bold cyan]Step 5: Validating Output[/bold cyan]", expand=False))
    
    is_valid = True
    
    # Check for expected keys
    expected_keys = ["prompt", "response", "model_name", "timestamp"]
    for key in expected_keys:
        if key not in response:
            console.print(f"  [red]âœ—[/red] Missing expected key in response: '{key}'")
            is_valid = False
    
    # Check model name
    if response.get("model_name") != OLLAMA_MODEL:
        console.print(f"  [red]âœ—[/red] Unexpected model name. Expected '{OLLAMA_MODEL}', got '{response.get('model_name')}'")
        is_valid = False
        
    # Check response content (simple check)
    if not response.get("response") or len(response["response"]) < 10:
        console.print(f"  [red]âœ—[/red] Response content is too short or empty.")
        is_valid = False
        
    if is_valid:
        console.print("  [green]âœ“[/green] Output validation successful.")
        console.print(f"  [green]Final Response:[/green] {response.get('response')[:100]}...")
    else:
        console.print("  [red]âœ—[/red] Output validation failed.")
        
    return is_valid

# --- Main Orchestration ---
async def main():
    console.print(Panel(
        "[bold green]ðŸš€ Starting Full Cycle: Weaver > Forge > Jinja > Pydantic AI > Ollama[/bold green]\n"
        "This demonstration validates the complete WeaverGen AI code generation pipeline.",
        title="WeaverGen Full Cycle Demo",
        border_style="green"
    ))

    # Clean up previous runs
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    TEMP_DIR.mkdir()

    try:
        semantic_file = define_semantic_convention()
        template_file = create_jinja2_template()
        generated_file = run_weaver_forge(semantic_file, template_file)
        ollama_response = await execute_generated_code(generated_file)
        validation_passed = validate_output(ollama_response)

        console.print(Panel("[bold cyan]Full Cycle Result[/bold cyan]", expand=False))
        if validation_passed:
            console.print("[bold green]âœ… Full cycle completed successfully and validated![/bold green]")
        else:
            console.print("[bold red]âœ— Full cycle failed validation.[/bold red]")

    except Exception as e:
        console.print(f"[bold red]âœ— An error occurred during the full cycle: {e}[/bold red]")
    finally:
        if TEMP_DIR.exists():
            shutil.rmtree(TEMP_DIR)
        console.print(f"\n[dim]ðŸ§¹ Cleaned up temporary directory: {TEMP_DIR}[/dim]")

if __name__ == "__main__":
    asyncio.run(main())